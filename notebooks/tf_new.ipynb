{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-28 11:47:38.966090: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-28 11:47:39.025026: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-28 11:47:39.026577: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-28 11:47:40.199229: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data.dat\", delimiter=\"\\t\", header=None, names=[\"target\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((df[\"text\"].tolist(), df[\"target\"].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(df)\n",
    "train_size = int(0.8*total)\n",
    "val_size = int(0.8*train_size)\n",
    "test_size = int(0.2*total)\n",
    "\n",
    "batch_size = 32\n",
    "seed=42\n",
    "\n",
    "raw_train_ds = dataset.skip(test_size)\n",
    "raw_test_ds = dataset.take(test_size)\n",
    "raw_train_ds = raw_train_ds.skip(val_size)\n",
    "raw_val_ds = raw_train_ds.take(val_size)\n",
    "\n",
    "raw_train_ds = raw_train_ds.batch(batch_size=batch_size, drop_remainder=True)\n",
    "raw_val_ds = raw_val_ds.batch(batch_size=batch_size, drop_remainder=True)\n",
    "raw_test_ds = raw_test_ds.batch(batch_size=batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation),\n",
    "                                  '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "sequence_length = 250\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a text-only dataset (without labels), then call adapt\n",
    "train_text = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return vectorize_layer(text), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review tf.Tensor(b'Interstitial chemotherapy with drug polymer implants for the treatment of recurrent gliomas. Malignant gliomas have been difficult to treat with chemotherapy. The most effective agent, BCNU (carmustine), has considerable systemic toxicity and a short half-life in serum. To obviate these problems, a method has been developed for the local sustained release of chemotherapeutic agents by their incorporation into biodegradable polymers. Implantation of the drug-impregnated polymer at the tumor site allows prolonged local exposure with minimal systemic exposure. In this Phase I-II study, 21 patients with recurrent malignant glioma were treated with BCNU released interstitially by means of a polyanhydride biodegradable polymer implant. Up to eight polymer wafers were placed in the resection cavity intraoperatively, upon completion of tumor debulking. The polymer releases the therapeutic drug for approximately 3 weeks. Three increasing concentrations of BCNU were studied; the treatment was well tolerated at all three levels. There were no adverse reactions to the BCNU wafer treatment itself. The average survival period after reoperation was 65 weeks for the first dose group, 64 weeks for the second dose group, and 32 weeks for the highest dose group. The overall mean survival time was 48 weeks from reoperation and 94 weeks from the original operation. The overall median survival times were 46 weeks postimplant and 87 weeks from initial surgery. Eighteen (86%) of 21 patients lived more than 1 year from the time of their initial diagnosis and eight (38%) of 21 patients lived more than 1 year after intracranial implantation of the polymer. Frequent hematology, blood chemistry, and urinalysis tests did not reveal any systemic effect from this interstitial chemotherapy. Since the therapy is well tolerated and safe, a placebo-controlled clinical trial has been started. The trial will measure the effect of the second treatment dose on survival of patients with recurrent malignant glioma. ', shape=(), dtype=string)\n",
      "Label tf.Tensor(1, shape=(), dtype=int32)\n",
      "Vectorized review (<tf.Tensor: shape=(1, 250), dtype=int64, numpy=\n",
      "array([[2361,  252,    6,  268, 4384, 3609,   12,    2,   35,    3,  273,\n",
      "        3225,  255, 3225,   36,   57, 1007,    8, 1847,    6,  252,    2,\n",
      "         100,  283,  911, 6231,    1,   71, 2389,  334,  759,    4,    7,\n",
      "        1102, 2899,    5,  154,    8,    1,   31,  925,    7,  329,   71,\n",
      "          57,  188,   12,    2,  360, 1101,  822,    3, 5001,  573,   16,\n",
      "         102, 4854,  178,    1,    1, 1325,    3,    2,    1, 4384,   19,\n",
      "           2,   76,  503, 2508,  592,  360,  756,    6, 1023,  334,  756,\n",
      "           5,   22,  525,  619,   37,  354,    9,    6,  273,  255, 2783,\n",
      "          11,   90,    6, 6231, 3334,    1,   16,  846,    3,    7,    1,\n",
      "           1, 4384, 3841,  676,    8,  306, 4384,    1,   11, 1461,    5,\n",
      "           2,  309, 1949, 4838, 1141, 4205,    3,   76,    1,    2, 4384,\n",
      "           1,    2,  645,  268,   12,  702,   72,  276,   70,  803,  344,\n",
      "           3, 6231,   11,  160,    2,   35,   10,  209, 1564,   19,   40,\n",
      "          70,   94,   80,   11,   43,  936,  915,    8,    2, 6231,    1,\n",
      "          35, 1989,    2,  437,  107,  191,   26, 1263,   10,  709,  276,\n",
      "          12,    2,  159,  284,   33, 1063,  276,   12,    2,  374,  284,\n",
      "          33,    4,  675,  276,   12,    2, 1271,  284,   33,    2,  420,\n",
      "          68,  107,  123,   10,  622,  276,   18, 1263,    4, 1557,  276,\n",
      "          18,    2, 2344,  395,    2,  420,  323,  107,  438,   11,  817,\n",
      "         276,    1,    4, 1842,  276,   18,  278,  105, 6097, 1369,    3,\n",
      "         354,    9, 6727,   52,   17,   50,  263,   18,    2,  123,    3,\n",
      "         102,  278,   99,    4,  306,  840,    3,  354,    9, 6727,   52,\n",
      "          17,   50,  263,   26,  802, 1325,    3,    2]])>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
     ]
    }
   ],
   "source": [
    "# retrieve a batch (of 32 reviews and labels) from the dataset\n",
    "text_batch, label_batch = next(iter(raw_train_ds))\n",
    "first_review, first_label = text_batch[0], label_batch[0]\n",
    "print(\"Review\", first_review)\n",
    "print (\"Label\", first_label)\n",
    "# print(\"Label\", raw_train_ds)\n",
    "print(\"Vectorized review\", vectorize_text(first_review, first_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1287 --->  staging\n",
      " 313 --->  wall\n",
      "Vocabulary size: 10000\n"
     ]
    }
   ],
   "source": [
    "print(\"1287 ---> \",vectorize_layer.get_vocabulary()[1287])\n",
    "print(\" 313 ---> \",vectorize_layer.get_vocabulary()[313])\n",
    "print('Vocabulary size: {}'.format(len(vectorize_layer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, None, 16)          160016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, None, 16)          0         \n",
      "                                                                 \n",
      " global_average_pooling1d (  (None, 16)                0         \n",
      " GlobalAveragePooling1D)                                         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 102       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160118 (625.46 KB)\n",
      "Trainable params: 160118 (625.46 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  layers.Embedding(max_features + 1, embedding_dim),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.GlobalAveragePooling1D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Dense(6, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer='adam',\n",
    "              metrics=tf.metrics.SparseCategoricalAccuracy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/admin/tfenv/lib/python3.10/site-packages/keras/src/backend.py:5714: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 1s 8ms/step - loss: 1.7455 - sparse_categorical_accuracy: 0.3255 - val_loss: 1.6949 - val_sparse_categorical_accuracy: 0.3433\n",
      "Epoch 2/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.6550 - sparse_categorical_accuracy: 0.3433 - val_loss: 1.6121 - val_sparse_categorical_accuracy: 0.3433\n",
      "Epoch 3/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.5900 - sparse_categorical_accuracy: 0.3433 - val_loss: 1.5657 - val_sparse_categorical_accuracy: 0.3433\n",
      "Epoch 4/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.5558 - sparse_categorical_accuracy: 0.3433 - val_loss: 1.5393 - val_sparse_categorical_accuracy: 0.3433\n",
      "Epoch 5/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.5320 - sparse_categorical_accuracy: 0.3433 - val_loss: 1.5180 - val_sparse_categorical_accuracy: 0.3433\n",
      "Epoch 6/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.5160 - sparse_categorical_accuracy: 0.3433 - val_loss: 1.4987 - val_sparse_categorical_accuracy: 0.3438\n",
      "Epoch 7/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.4974 - sparse_categorical_accuracy: 0.3455 - val_loss: 1.4800 - val_sparse_categorical_accuracy: 0.3455\n",
      "Epoch 8/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.4773 - sparse_categorical_accuracy: 0.3494 - val_loss: 1.4606 - val_sparse_categorical_accuracy: 0.3516\n",
      "Epoch 9/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.4586 - sparse_categorical_accuracy: 0.3637 - val_loss: 1.4407 - val_sparse_categorical_accuracy: 0.3689\n",
      "Epoch 10/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.4383 - sparse_categorical_accuracy: 0.3845 - val_loss: 1.4201 - val_sparse_categorical_accuracy: 0.3828\n",
      "Epoch 11/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.4151 - sparse_categorical_accuracy: 0.3937 - val_loss: 1.3981 - val_sparse_categorical_accuracy: 0.3976\n",
      "Epoch 12/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.3970 - sparse_categorical_accuracy: 0.4201 - val_loss: 1.3752 - val_sparse_categorical_accuracy: 0.4206\n",
      "Epoch 13/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.3763 - sparse_categorical_accuracy: 0.4384 - val_loss: 1.3517 - val_sparse_categorical_accuracy: 0.4314\n",
      "Epoch 14/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.3549 - sparse_categorical_accuracy: 0.4523 - val_loss: 1.3273 - val_sparse_categorical_accuracy: 0.4518\n",
      "Epoch 15/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.3302 - sparse_categorical_accuracy: 0.4714 - val_loss: 1.3025 - val_sparse_categorical_accuracy: 0.4757\n",
      "Epoch 16/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.3035 - sparse_categorical_accuracy: 0.4835 - val_loss: 1.2764 - val_sparse_categorical_accuracy: 0.4952\n",
      "Epoch 17/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.2734 - sparse_categorical_accuracy: 0.5039 - val_loss: 1.2499 - val_sparse_categorical_accuracy: 0.5126\n",
      "Epoch 18/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.2557 - sparse_categorical_accuracy: 0.5191 - val_loss: 1.2234 - val_sparse_categorical_accuracy: 0.5230\n",
      "Epoch 19/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.2220 - sparse_categorical_accuracy: 0.5378 - val_loss: 1.1964 - val_sparse_categorical_accuracy: 0.5499\n",
      "Epoch 20/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.2017 - sparse_categorical_accuracy: 0.5477 - val_loss: 1.1698 - val_sparse_categorical_accuracy: 0.5582\n",
      "Epoch 21/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.1752 - sparse_categorical_accuracy: 0.5551 - val_loss: 1.1432 - val_sparse_categorical_accuracy: 0.5664\n",
      "Epoch 22/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.1523 - sparse_categorical_accuracy: 0.5603 - val_loss: 1.1167 - val_sparse_categorical_accuracy: 0.5725\n",
      "Epoch 23/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.1229 - sparse_categorical_accuracy: 0.5820 - val_loss: 1.0900 - val_sparse_categorical_accuracy: 0.5825\n",
      "Epoch 24/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.1039 - sparse_categorical_accuracy: 0.5838 - val_loss: 1.0635 - val_sparse_categorical_accuracy: 0.5972\n",
      "Epoch 25/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.0732 - sparse_categorical_accuracy: 0.6033 - val_loss: 1.0373 - val_sparse_categorical_accuracy: 0.6076\n",
      "Epoch 26/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.0475 - sparse_categorical_accuracy: 0.6202 - val_loss: 1.0111 - val_sparse_categorical_accuracy: 0.6267\n",
      "Epoch 27/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 1.0187 - sparse_categorical_accuracy: 0.6276 - val_loss: 0.9850 - val_sparse_categorical_accuracy: 0.6380\n",
      "Epoch 28/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.9973 - sparse_categorical_accuracy: 0.6393 - val_loss: 0.9595 - val_sparse_categorical_accuracy: 0.6502\n",
      "Epoch 29/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.9723 - sparse_categorical_accuracy: 0.6545 - val_loss: 0.9343 - val_sparse_categorical_accuracy: 0.6667\n",
      "Epoch 30/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.9454 - sparse_categorical_accuracy: 0.6732 - val_loss: 0.9096 - val_sparse_categorical_accuracy: 0.6758\n",
      "Epoch 31/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.9254 - sparse_categorical_accuracy: 0.6814 - val_loss: 0.8850 - val_sparse_categorical_accuracy: 0.6905\n",
      "Epoch 32/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.8938 - sparse_categorical_accuracy: 0.6988 - val_loss: 0.8607 - val_sparse_categorical_accuracy: 0.7092\n",
      "Epoch 33/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.8766 - sparse_categorical_accuracy: 0.7109 - val_loss: 0.8371 - val_sparse_categorical_accuracy: 0.7201\n",
      "Epoch 34/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.8547 - sparse_categorical_accuracy: 0.7209 - val_loss: 0.8138 - val_sparse_categorical_accuracy: 0.7344\n",
      "Epoch 35/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.8309 - sparse_categorical_accuracy: 0.7418 - val_loss: 0.7911 - val_sparse_categorical_accuracy: 0.7478\n",
      "Epoch 36/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.8117 - sparse_categorical_accuracy: 0.7457 - val_loss: 0.7692 - val_sparse_categorical_accuracy: 0.7543\n",
      "Epoch 37/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 0.7878 - sparse_categorical_accuracy: 0.7574 - val_loss: 0.7477 - val_sparse_categorical_accuracy: 0.7643\n",
      "Epoch 38/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7621 - sparse_categorical_accuracy: 0.7639 - val_loss: 0.7265 - val_sparse_categorical_accuracy: 0.7765\n",
      "Epoch 39/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7438 - sparse_categorical_accuracy: 0.7778 - val_loss: 0.7059 - val_sparse_categorical_accuracy: 0.7908\n",
      "Epoch 40/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7280 - sparse_categorical_accuracy: 0.7821 - val_loss: 0.6862 - val_sparse_categorical_accuracy: 0.7977\n",
      "Epoch 41/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.7073 - sparse_categorical_accuracy: 0.7917 - val_loss: 0.6670 - val_sparse_categorical_accuracy: 0.8030\n",
      "Epoch 42/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.6922 - sparse_categorical_accuracy: 0.7951 - val_loss: 0.6484 - val_sparse_categorical_accuracy: 0.8099\n",
      "Epoch 43/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.6731 - sparse_categorical_accuracy: 0.7973 - val_loss: 0.6303 - val_sparse_categorical_accuracy: 0.8181\n",
      "Epoch 44/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.6573 - sparse_categorical_accuracy: 0.8082 - val_loss: 0.6124 - val_sparse_categorical_accuracy: 0.8260\n",
      "Epoch 45/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.6349 - sparse_categorical_accuracy: 0.8125 - val_loss: 0.5952 - val_sparse_categorical_accuracy: 0.8312\n",
      "Epoch 46/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.6157 - sparse_categorical_accuracy: 0.8320 - val_loss: 0.5787 - val_sparse_categorical_accuracy: 0.8329\n",
      "Epoch 47/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.6022 - sparse_categorical_accuracy: 0.8333 - val_loss: 0.5625 - val_sparse_categorical_accuracy: 0.8411\n",
      "Epoch 48/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5857 - sparse_categorical_accuracy: 0.8420 - val_loss: 0.5469 - val_sparse_categorical_accuracy: 0.8477\n",
      "Epoch 49/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5732 - sparse_categorical_accuracy: 0.8385 - val_loss: 0.5318 - val_sparse_categorical_accuracy: 0.8537\n",
      "Epoch 50/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5534 - sparse_categorical_accuracy: 0.8485 - val_loss: 0.5171 - val_sparse_categorical_accuracy: 0.8589\n",
      "Epoch 51/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5434 - sparse_categorical_accuracy: 0.8537 - val_loss: 0.5028 - val_sparse_categorical_accuracy: 0.8659\n",
      "Epoch 52/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5276 - sparse_categorical_accuracy: 0.8668 - val_loss: 0.4888 - val_sparse_categorical_accuracy: 0.8733\n",
      "Epoch 53/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5131 - sparse_categorical_accuracy: 0.8711 - val_loss: 0.4756 - val_sparse_categorical_accuracy: 0.8754\n",
      "Epoch 54/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.5016 - sparse_categorical_accuracy: 0.8724 - val_loss: 0.4625 - val_sparse_categorical_accuracy: 0.8824\n",
      "Epoch 55/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.4901 - sparse_categorical_accuracy: 0.8776 - val_loss: 0.4501 - val_sparse_categorical_accuracy: 0.8854\n",
      "Epoch 56/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.4744 - sparse_categorical_accuracy: 0.8763 - val_loss: 0.4378 - val_sparse_categorical_accuracy: 0.8915\n",
      "Epoch 57/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.4626 - sparse_categorical_accuracy: 0.8815 - val_loss: 0.4259 - val_sparse_categorical_accuracy: 0.8950\n",
      "Epoch 58/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.4515 - sparse_categorical_accuracy: 0.8906 - val_loss: 0.4145 - val_sparse_categorical_accuracy: 0.8976\n",
      "Epoch 59/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.4370 - sparse_categorical_accuracy: 0.8915 - val_loss: 0.4034 - val_sparse_categorical_accuracy: 0.9019\n",
      "Epoch 60/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.4259 - sparse_categorical_accuracy: 0.8928 - val_loss: 0.3924 - val_sparse_categorical_accuracy: 0.9071\n",
      "Epoch 61/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.4137 - sparse_categorical_accuracy: 0.8993 - val_loss: 0.3819 - val_sparse_categorical_accuracy: 0.9093\n",
      "Epoch 62/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.4037 - sparse_categorical_accuracy: 0.9110 - val_loss: 0.3717 - val_sparse_categorical_accuracy: 0.9106\n",
      "Epoch 63/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.3989 - sparse_categorical_accuracy: 0.9041 - val_loss: 0.3616 - val_sparse_categorical_accuracy: 0.9175\n",
      "Epoch 64/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.3906 - sparse_categorical_accuracy: 0.9049 - val_loss: 0.3524 - val_sparse_categorical_accuracy: 0.9171\n",
      "Epoch 65/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.3796 - sparse_categorical_accuracy: 0.9106 - val_loss: 0.3429 - val_sparse_categorical_accuracy: 0.9253\n",
      "Epoch 66/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.3687 - sparse_categorical_accuracy: 0.9145 - val_loss: 0.3340 - val_sparse_categorical_accuracy: 0.9240\n",
      "Epoch 67/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.3607 - sparse_categorical_accuracy: 0.9136 - val_loss: 0.3251 - val_sparse_categorical_accuracy: 0.9258\n",
      "Epoch 68/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.3484 - sparse_categorical_accuracy: 0.9180 - val_loss: 0.3168 - val_sparse_categorical_accuracy: 0.9258\n",
      "Epoch 69/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.3426 - sparse_categorical_accuracy: 0.9271 - val_loss: 0.3085 - val_sparse_categorical_accuracy: 0.9306\n",
      "Epoch 70/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.3356 - sparse_categorical_accuracy: 0.9219 - val_loss: 0.3006 - val_sparse_categorical_accuracy: 0.9332\n",
      "Epoch 71/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.3283 - sparse_categorical_accuracy: 0.9214 - val_loss: 0.2928 - val_sparse_categorical_accuracy: 0.9366\n",
      "Epoch 72/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.3155 - sparse_categorical_accuracy: 0.9266 - val_loss: 0.2854 - val_sparse_categorical_accuracy: 0.9379\n",
      "Epoch 73/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.3161 - sparse_categorical_accuracy: 0.9266 - val_loss: 0.2781 - val_sparse_categorical_accuracy: 0.9401\n",
      "Epoch 74/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.3037 - sparse_categorical_accuracy: 0.9288 - val_loss: 0.2712 - val_sparse_categorical_accuracy: 0.9418\n",
      "Epoch 75/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.2987 - sparse_categorical_accuracy: 0.9301 - val_loss: 0.2645 - val_sparse_categorical_accuracy: 0.9427\n",
      "Epoch 76/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.2909 - sparse_categorical_accuracy: 0.9345 - val_loss: 0.2579 - val_sparse_categorical_accuracy: 0.9444\n",
      "Epoch 77/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.2830 - sparse_categorical_accuracy: 0.9405 - val_loss: 0.2517 - val_sparse_categorical_accuracy: 0.9440\n",
      "Epoch 78/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.2763 - sparse_categorical_accuracy: 0.9401 - val_loss: 0.2454 - val_sparse_categorical_accuracy: 0.9457\n",
      "Epoch 79/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.2750 - sparse_categorical_accuracy: 0.9345 - val_loss: 0.2398 - val_sparse_categorical_accuracy: 0.9453\n",
      "Epoch 80/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.2646 - sparse_categorical_accuracy: 0.9388 - val_loss: 0.2339 - val_sparse_categorical_accuracy: 0.9466\n",
      "Epoch 81/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.2570 - sparse_categorical_accuracy: 0.9397 - val_loss: 0.2283 - val_sparse_categorical_accuracy: 0.9479\n",
      "Epoch 82/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.2533 - sparse_categorical_accuracy: 0.9423 - val_loss: 0.2230 - val_sparse_categorical_accuracy: 0.9497\n",
      "Epoch 83/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.2486 - sparse_categorical_accuracy: 0.9427 - val_loss: 0.2177 - val_sparse_categorical_accuracy: 0.9523\n",
      "Epoch 84/100\n",
      "72/72 [==============================] - 0s 6ms/step - loss: 0.2498 - sparse_categorical_accuracy: 0.9431 - val_loss: 0.2127 - val_sparse_categorical_accuracy: 0.9510\n",
      "Epoch 85/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.2337 - sparse_categorical_accuracy: 0.9531 - val_loss: 0.2078 - val_sparse_categorical_accuracy: 0.9536\n",
      "Epoch 86/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.2299 - sparse_categorical_accuracy: 0.9457 - val_loss: 0.2032 - val_sparse_categorical_accuracy: 0.9527\n",
      "Epoch 87/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.2261 - sparse_categorical_accuracy: 0.9479 - val_loss: 0.1986 - val_sparse_categorical_accuracy: 0.9544\n",
      "Epoch 88/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.2222 - sparse_categorical_accuracy: 0.9479 - val_loss: 0.1942 - val_sparse_categorical_accuracy: 0.9549\n",
      "Epoch 89/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.2201 - sparse_categorical_accuracy: 0.9470 - val_loss: 0.1900 - val_sparse_categorical_accuracy: 0.9562\n",
      "Epoch 90/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.2161 - sparse_categorical_accuracy: 0.9544 - val_loss: 0.1857 - val_sparse_categorical_accuracy: 0.9557\n",
      "Epoch 91/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.2103 - sparse_categorical_accuracy: 0.9510 - val_loss: 0.1818 - val_sparse_categorical_accuracy: 0.9562\n",
      "Epoch 92/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.2046 - sparse_categorical_accuracy: 0.9540 - val_loss: 0.1780 - val_sparse_categorical_accuracy: 0.9562\n",
      "Epoch 93/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.2021 - sparse_categorical_accuracy: 0.9501 - val_loss: 0.1740 - val_sparse_categorical_accuracy: 0.9562\n",
      "Epoch 94/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.1980 - sparse_categorical_accuracy: 0.9505 - val_loss: 0.1704 - val_sparse_categorical_accuracy: 0.9557\n",
      "Epoch 95/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.1914 - sparse_categorical_accuracy: 0.9557 - val_loss: 0.1670 - val_sparse_categorical_accuracy: 0.9566\n",
      "Epoch 96/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.1906 - sparse_categorical_accuracy: 0.9505 - val_loss: 0.1636 - val_sparse_categorical_accuracy: 0.9570\n",
      "Epoch 97/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.1887 - sparse_categorical_accuracy: 0.9527 - val_loss: 0.1604 - val_sparse_categorical_accuracy: 0.9575\n",
      "Epoch 98/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.1827 - sparse_categorical_accuracy: 0.9501 - val_loss: 0.1573 - val_sparse_categorical_accuracy: 0.9579\n",
      "Epoch 99/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.1803 - sparse_categorical_accuracy: 0.9540 - val_loss: 0.1542 - val_sparse_categorical_accuracy: 0.9579\n",
      "Epoch 100/100\n",
      "72/72 [==============================] - 0s 5ms/step - loss: 0.1751 - sparse_categorical_accuracy: 0.9562 - val_loss: 0.1510 - val_sparse_categorical_accuracy: 0.9583\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = raw_train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = raw_val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = raw_test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT model selected           : https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Preprocess model auto-selected: https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\n"
     ]
    }
   ],
   "source": [
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys       : ['input_word_ids', 'input_mask', 'input_type_ids']\n",
      "Shape      : (1, 128)\n",
      "Word Ids   : [ 101 2023 2003 2107 2019 6429 3185  999  102    0    0    0]\n",
      "Input Mask : [1 1 1 1 1 1 1 1 1 0 0 0]\n",
      "Type Ids   : [0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "text_test = ['this is such an amazing movie!']\n",
    "text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded BERT: https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Pooled Outputs Shape:(1, 512)\n",
      "Pooled Outputs Values:[ 0.762629    0.99280983 -0.18611868  0.36673862  0.15233733  0.6550447\n",
      "  0.9681154  -0.9486271   0.00216128 -0.9877732   0.06842692 -0.97630584]\n",
      "Sequence Outputs Shape:(1, 128, 512)\n",
      "Sequence Outputs Values:[[-0.28946346  0.3432128   0.33231518 ...  0.21300825  0.7102068\n",
      "  -0.05771117]\n",
      " [-0.28742072  0.31981036 -0.23018576 ...  0.58455    -0.21329743\n",
      "   0.72692114]\n",
      " [-0.66157067  0.68876773 -0.8743301  ...  0.1087725  -0.26173177\n",
      "   0.47855407]\n",
      " ...\n",
      " [-0.2256118  -0.2892561  -0.0706445  ...  0.47566038  0.83277136\n",
      "   0.40025333]\n",
      " [-0.2982428  -0.27473134 -0.05450517 ...  0.48849747  1.0955354\n",
      "   0.18163396]\n",
      " [-0.44378242  0.00930811  0.07223688 ...  0.1729009   1.1833243\n",
      "   0.07898017]]\n"
     ]
    }
   ],
   "source": [
    "bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  net = outputs['pooled_output']\n",
    "  net = tf.keras.layers.Dropout(0.1)(net)\n",
    "  net = tf.keras.layers.Dense(6, activation=\"softmax\", name='classifier')(net)\n",
    "  return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.53741455 0.50679404 0.52888566 0.5321198  0.6067604  0.53614706]], shape=(1, 6), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "classifier_model = build_classifier_model()\n",
    "bert_raw_result = classifier_model(tf.constant(text_test))\n",
    "print(tf.sigmoid(bert_raw_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(classifier_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "metrics = tf.metrics.SparseCategoricalAccuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_model.compile(optimizer=optimizer,\n",
    "                         loss=loss,\n",
    "                         metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72/72 [==============================] - 156s 2s/step - loss: 1.7031 - sparse_categorical_accuracy: 0.2869 - val_loss: 1.3355 - val_sparse_categorical_accuracy: 0.4796\n",
      "Epoch 2/5\n",
      "72/72 [==============================] - 149s 2s/step - loss: 1.2379 - sparse_categorical_accuracy: 0.4935 - val_loss: 0.9872 - val_sparse_categorical_accuracy: 0.6172\n",
      "Epoch 3/5\n",
      "72/72 [==============================] - 148s 2s/step - loss: 1.0237 - sparse_categorical_accuracy: 0.5907 - val_loss: 0.8499 - val_sparse_categorical_accuracy: 0.6732\n",
      "Epoch 4/5\n",
      "72/72 [==============================] - 149s 2s/step - loss: 0.9124 - sparse_categorical_accuracy: 0.6441 - val_loss: 0.8024 - val_sparse_categorical_accuracy: 0.6931\n",
      "Epoch 5/5\n",
      "72/72 [==============================] - 148s 2s/step - loss: 0.8465 - sparse_categorical_accuracy: 0.6671 - val_loss: 0.7474 - val_sparse_categorical_accuracy: 0.7096\n"
     ]
    }
   ],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "history = classifier_model.fit(x=train_ds,\n",
    "                               validation_data=val_ds,\n",
    "                               epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 44s 484ms/step - loss: 1.0469 - sparse_categorical_accuracy: 0.5833\n",
      "Loss: 1.0468590259552002\n",
      "Accuracy: 0.5833333134651184\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = classifier_model.evaluate(test_ds)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_dict = history.history\n",
    "# print(history_dict.keys())\n",
    "\n",
    "# acc = history_dict['binary_accuracy']\n",
    "# val_acc = history_dict['val_binary_accuracy']\n",
    "# loss = history_dict['loss']\n",
    "# val_loss = history_dict['val_loss']\n",
    "\n",
    "# epochs = range(1, len(acc) + 1)\n",
    "# fig = plt.figure(figsize=(10, 6))\n",
    "# fig.tight_layout()\n",
    "\n",
    "# plt.subplot(2, 1, 1)\n",
    "# # r is for \"solid red line\"\n",
    "# plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# # b is for \"solid blue line\"\n",
    "# plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# # plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.subplot(2, 1, 2)\n",
    "# plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "# plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "# plt.title('Training and validation accuracy')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./inveni_bert/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./inveni_bert/assets\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'inveni'\n",
    "saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n",
    "\n",
    "classifier_model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = df[\"text\"].tolist()[:5]\n",
    "\n",
    "reloaded_results = tf.keras.activations.softmax(reloaded_model(tf.constant(examples)))\n",
    "reloaded_results = tf.reduce_sum(reloaded_results[0, :])\n",
    "original_results = tf.keras.activations.softmax(classifier_model(tf.constant(examples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reloaded_results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_my_examples(inputs, results):\n",
    "  result_for_printing = \\\n",
    "    [f'input: {inputs[i]:<30} : score: {results[i][0]:.6f}'\n",
    "                         for i in range(len(inputs))]\n",
    "  print(*result_for_printing, sep='\\n')\n",
    "  print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results from the saved model:\n",
      "input: Catheterization laboratory events and hospital outcome with direct angioplasty for acute myocardial infarction To assess the safety of direct infarct angioplasty without antecedent thrombolytic therapy, catheterization laboratory and hospital events were assessed in consecutively treated patients with infarctions involving the left anterior descending (n = 100 patients), right (n = 100), and circumflex (n = 50) coronary arteries. The groups of patients were similar for age (left anterior descending coronary artery, 59 years; right coronary artery, 58 years; circumflex coronary artery, 62 years), patients with multivessel disease (left anterior descending coronary artery, 55%; right coronary artery, 55%; circumflex coronary artery, 64%), and patients with initial grade 0/1 antegrade flow (left anterior descending coronary artery, 79%; right coronary artery, 84%; circumflex coronary artery, 90%). Cardiogenic shock was present in eight patients with infarction of the left anterior descending coronary artery, four with infarction of the right coronary artery, and four with infarction of the circumflex coronary artery. Major catheterization laboratory events (cardioversion, cardiopulmonary resuscitation, dopamine or intra-aortic balloon pump support for hypotension, and urgent surgery) occurred in 10 patients with infarction of the left anterior descending coronary artery, eight with infarction of the right coronary artery, and four with infarction of the circumflex coronary artery (16 of 16 shock and six of 234 nonshock patients, p less than 0.001). There was one in-laboratory death (shock patient with infarction of the left anterior descending coronary artery).  : score: 0.133228\n",
      "input: Renal abscess in children. Three cases of renal abscesses in children are described to illustrate the variable presenting features. An additional 23 pediatric cases, reported over the past ten years, were reviewed for clinical features and therapy. Fever, loin pain, and leukocytosis were common presenting features, but less than half of all abscesses were associated with either an abnormal urinalysis or a positive urine culture. The presenting features were sometimes confused with appendicitis, peritonitis, or a Wilms tumor. An organism was identified in 17 cases--Escherichia coli in 9 children and Staphylococcus aureus in 8 children. The majority of E. coli infections occurred in girls and the majority of S. aureus infections occurred in boys. Reflux was documented in 5 patients, and 2 children had a possible extrarenal source of infection. Antibiotics alone produced a cure in 10 children (38%), but 16 children (62%) required a surgical procedure.  : score: 0.139271\n",
      "input: Hyperplastic polyps seen at sigmoidoscopy are markers for additional adenomas seen at colonoscopy. Asymptomatic individuals undergoing screening flexible sigmoidoscopy were prospectively studied. Polyps were found in 185 subjects. The endoscopist recorded an opinion on the polyps' histology based on endoscopic appearance. No polyps were removed at sigmoidoscopy. All subjects with rectosigmoid polyps then underwent colonoscopy and polypectomy. Of them, 99 subjects (54%) had at least one rectosigmoid adenoma, 69 (37%) had only hyperplastic polyps, and 17 (9%) had other findings. The endoscopists' opinion of the histopathology of polyps at sigmoidoscopy was correct for 61% of the lesions. Of subjects with adenomatous rectosigmoid polyps, 29% had additional adenomas at more proximal sites. Proximal adenomas were found in 28% of patients with hyperplastic rectosigmoid polyps. Patients with rectosigmoid hyperplastic polyps had the same risk for additional proximal adenomas as patients with rectosigmoid adenomatous polyps.  : score: 0.136501\n",
      "input: Subclavian artery to innominate vein fistula after insertion of a hemodialysis catheter. Insertion of hemodialysis catheters for temporary use is now preferentially performed by percutaneous infraclavicular subclavian vein catheterization. This method involves passage of a stiff dilator and a peel-away sheath over a guide wire, and is usually carried out without fluoroscopy. For the most part this has proved to be a valuable and safe approach. However, a small incidence of major complications occurs, which needs to be emphasized. Sixteen cases of arteriovenous fistulas between the subclavian artery or its branches and the subclavian vein have been reported so far in the literature. To date only one case of subclavian artery to innominate vein fistula has been reported. We report the second case with this complication and suggest possible preventive measures.  : score: 0.137980\n",
      "input: Effect of local inhibition of gamma-aminobutyric acid uptake in the dorsomedial hypothalamus on extracellular levels of gamma-aminobutyric acid and on stress-induced tachycardia: a study using microdialysis. Previous studies involving local microinjection of drugs that interfere with gamma-aminobutyric acid (GABA)A receptor-mediated synaptic inhibition have led to the suggestion that endogenous GABA suppresses the activity of a sympatho-excitatory mechanism in the dorsomedial hypothalamus in rats. In this study, microdialysis was used to assess and to alter pharmacologically extracellular-levels of GABA within this region while simultaneously monitoring heart rate and blood pressure. In anesthetized rats, local microdialysis for 15 min with 2.5, 10 and 40 mM nipecotic acid, an inhibitor of GABA uptake, caused concentration-related increases in GABA and taurine in the extracellular space, but no significant change in heart rate or arterial pressure. Similar perfusion with 37.5, 75 and 150 mM KCl caused concentration-related increases in GABA as well as aspartate, glutamate, taurine, glycine and alanine. Only modest, variable increases in heart rate and no effect on arterial pressure were observed during the perfusions with high potassium. In conscious rats, unilateral microdialysis of the dorsomedial hypothalamus with 0.5 mM nipecotic acid for 2 to 2.5 hr before stress coupled with contralateral microinjection of muscimol (88 pmol/250 nl) 5 min before stress significantly reduced air stress-induced tachycardia; this reduction in tachycardia was associated with markedly elevated levels of GABA in dialysates collected from the dorsomedial hypothalamus. Neither treatment alone significantly influenced stress-induced increases in heart rate, although perfusion with nipecotic acid alone evoked similar elevations in extracellular GABA. These results suggest that extracellular levels of endogenous GABA in the dorsomedial hypothalamus may regulate the cardiovascular response to stress.  : score: 0.139721\n",
      "\n",
      "Results from the model in memory:\n",
      "input: Catheterization laboratory events and hospital outcome with direct angioplasty for acute myocardial infarction To assess the safety of direct infarct angioplasty without antecedent thrombolytic therapy, catheterization laboratory and hospital events were assessed in consecutively treated patients with infarctions involving the left anterior descending (n = 100 patients), right (n = 100), and circumflex (n = 50) coronary arteries. The groups of patients were similar for age (left anterior descending coronary artery, 59 years; right coronary artery, 58 years; circumflex coronary artery, 62 years), patients with multivessel disease (left anterior descending coronary artery, 55%; right coronary artery, 55%; circumflex coronary artery, 64%), and patients with initial grade 0/1 antegrade flow (left anterior descending coronary artery, 79%; right coronary artery, 84%; circumflex coronary artery, 90%). Cardiogenic shock was present in eight patients with infarction of the left anterior descending coronary artery, four with infarction of the right coronary artery, and four with infarction of the circumflex coronary artery. Major catheterization laboratory events (cardioversion, cardiopulmonary resuscitation, dopamine or intra-aortic balloon pump support for hypotension, and urgent surgery) occurred in 10 patients with infarction of the left anterior descending coronary artery, eight with infarction of the right coronary artery, and four with infarction of the circumflex coronary artery (16 of 16 shock and six of 234 nonshock patients, p less than 0.001). There was one in-laboratory death (shock patient with infarction of the left anterior descending coronary artery).  : score: 0.133228\n",
      "input: Renal abscess in children. Three cases of renal abscesses in children are described to illustrate the variable presenting features. An additional 23 pediatric cases, reported over the past ten years, were reviewed for clinical features and therapy. Fever, loin pain, and leukocytosis were common presenting features, but less than half of all abscesses were associated with either an abnormal urinalysis or a positive urine culture. The presenting features were sometimes confused with appendicitis, peritonitis, or a Wilms tumor. An organism was identified in 17 cases--Escherichia coli in 9 children and Staphylococcus aureus in 8 children. The majority of E. coli infections occurred in girls and the majority of S. aureus infections occurred in boys. Reflux was documented in 5 patients, and 2 children had a possible extrarenal source of infection. Antibiotics alone produced a cure in 10 children (38%), but 16 children (62%) required a surgical procedure.  : score: 0.139271\n",
      "input: Hyperplastic polyps seen at sigmoidoscopy are markers for additional adenomas seen at colonoscopy. Asymptomatic individuals undergoing screening flexible sigmoidoscopy were prospectively studied. Polyps were found in 185 subjects. The endoscopist recorded an opinion on the polyps' histology based on endoscopic appearance. No polyps were removed at sigmoidoscopy. All subjects with rectosigmoid polyps then underwent colonoscopy and polypectomy. Of them, 99 subjects (54%) had at least one rectosigmoid adenoma, 69 (37%) had only hyperplastic polyps, and 17 (9%) had other findings. The endoscopists' opinion of the histopathology of polyps at sigmoidoscopy was correct for 61% of the lesions. Of subjects with adenomatous rectosigmoid polyps, 29% had additional adenomas at more proximal sites. Proximal adenomas were found in 28% of patients with hyperplastic rectosigmoid polyps. Patients with rectosigmoid hyperplastic polyps had the same risk for additional proximal adenomas as patients with rectosigmoid adenomatous polyps.  : score: 0.136501\n",
      "input: Subclavian artery to innominate vein fistula after insertion of a hemodialysis catheter. Insertion of hemodialysis catheters for temporary use is now preferentially performed by percutaneous infraclavicular subclavian vein catheterization. This method involves passage of a stiff dilator and a peel-away sheath over a guide wire, and is usually carried out without fluoroscopy. For the most part this has proved to be a valuable and safe approach. However, a small incidence of major complications occurs, which needs to be emphasized. Sixteen cases of arteriovenous fistulas between the subclavian artery or its branches and the subclavian vein have been reported so far in the literature. To date only one case of subclavian artery to innominate vein fistula has been reported. We report the second case with this complication and suggest possible preventive measures.  : score: 0.137980\n",
      "input: Effect of local inhibition of gamma-aminobutyric acid uptake in the dorsomedial hypothalamus on extracellular levels of gamma-aminobutyric acid and on stress-induced tachycardia: a study using microdialysis. Previous studies involving local microinjection of drugs that interfere with gamma-aminobutyric acid (GABA)A receptor-mediated synaptic inhibition have led to the suggestion that endogenous GABA suppresses the activity of a sympatho-excitatory mechanism in the dorsomedial hypothalamus in rats. In this study, microdialysis was used to assess and to alter pharmacologically extracellular-levels of GABA within this region while simultaneously monitoring heart rate and blood pressure. In anesthetized rats, local microdialysis for 15 min with 2.5, 10 and 40 mM nipecotic acid, an inhibitor of GABA uptake, caused concentration-related increases in GABA and taurine in the extracellular space, but no significant change in heart rate or arterial pressure. Similar perfusion with 37.5, 75 and 150 mM KCl caused concentration-related increases in GABA as well as aspartate, glutamate, taurine, glycine and alanine. Only modest, variable increases in heart rate and no effect on arterial pressure were observed during the perfusions with high potassium. In conscious rats, unilateral microdialysis of the dorsomedial hypothalamus with 0.5 mM nipecotic acid for 2 to 2.5 hr before stress coupled with contralateral microinjection of muscimol (88 pmol/250 nl) 5 min before stress significantly reduced air stress-induced tachycardia; this reduction in tachycardia was associated with markedly elevated levels of GABA in dialysates collected from the dorsomedial hypothalamus. Neither treatment alone significantly influenced stress-induced increases in heart rate, although perfusion with nipecotic acid alone evoked similar elevations in extracellular GABA. These results suggest that extracellular levels of endogenous GABA in the dorsomedial hypothalamus may regulate the cardiovascular response to stress.  : score: 0.139721\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Results from the saved model:')\n",
    "print_my_examples(examples, reloaded_results)\n",
    "print('Results from the model in memory:')\n",
    "print_my_examples(examples, original_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
